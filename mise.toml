[tools]
"github:pgschema/pgschema" = "1.7.2"
node = "22"

[tasks.generate-models]
description = "Generate Pydantic models from sureify.json OpenAPI spec"
run = "uv run datamodel-codegen --input sureify.yaml --output-model-type pydantic_v2.BaseModel --output api/src/api/sureify_models.py"

# =============================================================================
# pgschema Tasks
# =============================================================================

[tasks."pgschema:plan"]
description = "Plan schema changes for hackathon database"
run = "pgschema plan --schema hackathon --file database/main.sql"

[tasks."pgschema:apply"]
description = "Apply schema changes for hackathon database"
run = "pgschema apply --schema hackathon --file database/main.sql"

# =============================================================================
# ECR Build & Push
# =============================================================================

[tasks."ecr:login"]
description = "Login to ECR"
run = """
#!/bin/bash
set -euo pipefail
AWS_REGION="${AWS_REGION:-us-east-1}"
AWS_ACCOUNT_ID="${AWS_ACCOUNT_ID:-$(aws sts get-caller-identity --query Account --output text)}"
aws ecr get-login-password --region "$AWS_REGION" | \
  docker login --username AWS --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com"
"""

[tasks."ecr:create"]
description = "Create ECR repository for hack2future"
run = """
#!/bin/bash
set -euo pipefail
AWS_REGION="${AWS_REGION:-us-east-1}"
REPO_NAME="${ECR_REPO_NAME:-hack2future}"

aws ecr create-repository \
  --repository-name "$REPO_NAME" \
  --region "$AWS_REGION" \
  --image-scanning-configuration scanOnPush=true \
  --encryption-configuration encryptionType=AES256 \
  2>/dev/null || echo "Repository $REPO_NAME already exists"

echo "ECR repository ready: $REPO_NAME"
"""

[tasks."ecr:build"]
description = "Build API Docker image for ECR"
run = """
#!/bin/bash
set -euo pipefail
AWS_REGION="${AWS_REGION:-us-east-1}"
AWS_ACCOUNT_ID="${AWS_ACCOUNT_ID:-$(aws sts get-caller-identity --query Account --output text)}"
REPO_NAME="${ECR_REPO_NAME:-hack2future}"
TAG="${IMAGE_TAG:-latest}"
REGISTRY="$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com"

echo "Building $REPO_NAME:$TAG for linux/amd64..."
docker build --platform linux/amd64 -t "$REPO_NAME:$TAG" -f api/Dockerfile .
docker tag "$REPO_NAME:$TAG" "$REGISTRY/$REPO_NAME:$TAG"

echo "Image built and tagged: $REGISTRY/$REPO_NAME:$TAG"
"""

[tasks."ecr:push"]
description = "Push API Docker image to ECR"
depends = ["ecr:login"]
run = """
#!/bin/bash
set -euo pipefail
AWS_REGION="${AWS_REGION:-us-east-1}"
AWS_ACCOUNT_ID="${AWS_ACCOUNT_ID:-$(aws sts get-caller-identity --query Account --output text)}"
REPO_NAME="${ECR_REPO_NAME:-hack2future}"
TAG="${IMAGE_TAG:-latest}"
REGISTRY="$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com"

echo "Pushing $REGISTRY/$REPO_NAME:$TAG..."
docker push "$REGISTRY/$REPO_NAME:$TAG"

echo "Image pushed successfully!"
"""

[tasks."ecr:build-push"]
description = "Build and push API Docker image to ECR"
depends = ["ecr:build", "ecr:push"]

# =============================================================================
# Frontend ECR Build & Push
# =============================================================================

[tasks."frontend:ecr:create"]
description = "Create ECR repository for frontend"
run = """
#!/bin/bash
set -euo pipefail
AWS_REGION="${AWS_REGION:-us-east-1}"
REPO_NAME="hack2future-frontend"

aws ecr create-repository \
  --repository-name "$REPO_NAME" \
  --region "$AWS_REGION" \
  --image-scanning-configuration scanOnPush=true \
  --encryption-configuration encryptionType=AES256 \
  2>/dev/null || echo "Repository $REPO_NAME already exists"

echo "ECR repository ready: $REPO_NAME"
"""

[tasks."frontend:build"]
description = "Build frontend Docker image for ECR"
run = """
#!/bin/bash
set -euo pipefail
AWS_REGION="${AWS_REGION:-us-east-1}"
AWS_ACCOUNT_ID="${AWS_ACCOUNT_ID:-$(aws sts get-caller-identity --query Account --output text)}"
REPO_NAME="hack2future-frontend"
TAG="${IMAGE_TAG:-latest}"
REGISTRY="$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com"

echo "Building $REPO_NAME:$TAG for linux/amd64..."
docker build --platform linux/amd64 -t "$REPO_NAME:$TAG" -f frontend/Dockerfile .
docker tag "$REPO_NAME:$TAG" "$REGISTRY/$REPO_NAME:$TAG"

echo "Image built and tagged: $REGISTRY/$REPO_NAME:$TAG"
"""

[tasks."frontend:push"]
description = "Push frontend Docker image to ECR"
depends = ["ecr:login", "frontend:build"]
run = """
#!/bin/bash
set -euo pipefail
AWS_REGION="${AWS_REGION:-us-east-1}"
AWS_ACCOUNT_ID="${AWS_ACCOUNT_ID:-$(aws sts get-caller-identity --query Account --output text)}"
REPO_NAME="hack2future-frontend"
TAG="${IMAGE_TAG:-latest}"
REGISTRY="$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com"

echo "Pushing $REGISTRY/$REPO_NAME:$TAG..."
docker push "$REGISTRY/$REPO_NAME:$TAG"

echo "Image pushed successfully!"
"""

[tasks."frontend:build-push"]
description = "Build and push frontend Docker image to ECR"
depends = ["frontend:build", "frontend:push"]

# =============================================================================
# EKS Cluster Management
# =============================================================================

[tasks."eks:create"]
description = "Create EKS cluster with Auto Mode"
run = """
#!/bin/bash
set -euo pipefail
echo "Creating EKS cluster with Auto Mode..."
eksctl create cluster -f k8s/eksctl-cluster.yaml
echo "Cluster created successfully!"
"""

[tasks."eks:delete"]
description = "Delete EKS cluster"
run = """
#!/bin/bash
set -euo pipefail
echo "Deleting EKS cluster..."
eksctl delete cluster -f k8s/eksctl-cluster.yaml --wait
echo "Cluster deleted."
"""

[tasks."eks:status"]
description = "Check EKS cluster status"
run = """
#!/bin/bash
set -euo pipefail
eksctl get cluster -f k8s/eksctl-cluster.yaml 2>/dev/null || echo "Cluster not found"
echo ""
echo "Nodes:"
kubectl get nodes 2>/dev/null || echo "Cannot connect to cluster"
"""

[tasks."eks:kubeconfig"]
description = "Update kubeconfig for EKS cluster"
run = """
#!/bin/bash
set -euo pipefail
CLUSTER_NAME=$(grep -A2 'metadata:' k8s/eksctl-cluster.yaml | grep 'name:' | awk '{print $2}')
REGION=$(grep -A2 'metadata:' k8s/eksctl-cluster.yaml | grep 'region:' | awk '{print $2}')
aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$REGION"
echo "Kubeconfig updated. Current context:"
kubectl config current-context
"""

# =============================================================================
# EKS Deploy Tasks
# =============================================================================

[tasks."eks:deploy"]
description = "Deploy everything to EKS (CNPG + DB + App + Agents + Frontend + Gateway)"
depends = ["eks:deploy:cnpg", "eks:deploy:db", "eks:deploy:app", "eks:deploy:agents", "eks:deploy:frontend", "eks:deploy:gateway"]

[tasks."eks:deploy:agents"]
description = "Deploy agents"
run = """
#!/bin/bash
set -euo pipefail

echo "Creating agents service account..."
kubectl apply -f k8s/agents-rbac.yaml

echo "Deploying agents..."
kubectl apply -f k8s/agents.yaml

echo "Waiting for agents deployment..."
kubectl rollout status deployment/agents --timeout=120s

echo "Agents deployed!"
kubectl get pods -l app=agents
"""

[tasks."eks:deploy:cnpg"]
description = "Install CloudNativePG operator"
run = """
#!/bin/bash
set -euo pipefail
CNPG_VERSION="${CNPG_VERSION:-1.28.1}"

echo "Installing CloudNativePG v${CNPG_VERSION}..."
kubectl apply --server-side -f \
  "https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.28/releases/cnpg-${CNPG_VERSION}.yaml"

echo "Waiting for CNPG operator..."
kubectl wait --for=condition=Available deployment/cnpg-controller-manager \
  -n cnpg-system --timeout=300s

echo "CNPG operator ready!"
"""

[tasks."eks:deploy:db"]
description = "Deploy PostgreSQL cluster"
run = """
#!/bin/bash
set -euo pipefail

echo "Creating pgschema password secret..."
kubectl create secret generic pgschema-password \
  --from-literal=username=pgschema \
  --from-literal=password="$(openssl rand -base64 24)" \
  --dry-run=client -o yaml | kubectl apply -f -

echo "Deploying PostgreSQL cluster..."
kubectl apply -f k8s/dev-cluster.yaml

echo "Waiting for PostgreSQL cluster to be ready..."
kubectl wait --for=condition=Ready cluster/hackdb --timeout=600s

echo "PostgreSQL cluster ready!"
kubectl get cluster hackdb
"""

[tasks."eks:deploy:app"]
description = "Deploy API"
run = """
#!/bin/bash
set -euo pipefail

echo "Deploying API..."
kubectl apply -f k8s/api.yaml

echo "Waiting for deployment..."
kubectl rollout status deployment/hack2future --timeout=120s

echo  "API deployed!"
kubectl get pods -l app=hack2future
"""

[tasks."eks:deploy:frontend"]
description = "Deploy frontend"
run = """
#!/bin/bash
set -euo pipefail

echo "Deploying frontend..."
kubectl apply -f k8s/frontend.yaml

echo "Waiting for deployment..."
kubectl rollout status deployment/hack2future-frontend --timeout=120s

echo "Frontend deployed!"
kubectl get pods -l app=hack2future-frontend
"""

[tasks."eks:deploy:lbc"]
description = "Install AWS Load Balancer Controller with Gateway API support"
run = """
#!/bin/bash
set -euo pipefail
LBC_VERSION="${LBC_VERSION:-3.1.0}"
CLUSTER_NAME="${CLUSTER_NAME:-hackathon}"
AWS_REGION="${AWS_REGION:-us-east-1}"
POLICY_NAME="AWSLoadBalancerControllerIAMPolicy"

echo "Getting VPC ID and Account ID..."
VPC_ID=$(aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" --query 'cluster.resourcesVpcConfig.vpcId' --output text)
AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
echo "VPC ID: $VPC_ID"
echo "Account: $AWS_ACCOUNT_ID"

# Create IAM policy if it doesn't exist
echo "Checking IAM policy..."
POLICY_ARN="arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${POLICY_NAME}"
if ! aws iam get-policy --policy-arn "$POLICY_ARN" &>/dev/null; then
  echo "Creating IAM policy..."
  curl -sS -o /tmp/iam-policy.json \
    "https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v${LBC_VERSION}/docs/install/iam_policy.json"
  aws iam create-policy --policy-name "$POLICY_NAME" --policy-document file:///tmp/iam-policy.json
else
  echo "IAM policy already exists"
fi

# Create Pod Identity association with the policy
echo "Setting up Pod Identity..."
eksctl create podidentityassociation \
  --cluster "$CLUSTER_NAME" \
  --region "$AWS_REGION" \
  --namespace kube-system \
  --service-account-name aws-load-balancer-controller \
  --permission-policy-arns "$POLICY_ARN" \
  2>/dev/null || echo "Pod Identity association exists, updating..."

# If association exists, ensure the policy is attached to the role
ASSOC_ROLE=$(aws eks list-pod-identity-associations --cluster-name "$CLUSTER_NAME" --region "$AWS_REGION" \
  --query "associations[?serviceAccount=='aws-load-balancer-controller'].associationId" --output text)
if [[ -n "$ASSOC_ROLE" ]]; then
  ROLE_ARN=$(aws eks describe-pod-identity-association --cluster-name "$CLUSTER_NAME" --association-id "$ASSOC_ROLE" --region "$AWS_REGION" \
    --query 'association.roleArn' --output text)
  ROLE_NAME=$(basename "$ROLE_ARN")
  echo "Ensuring policy is attached to role: $ROLE_NAME"
  aws iam attach-role-policy --role-name "$ROLE_NAME" --policy-arn "$POLICY_ARN" 2>/dev/null || true
fi

echo "Adding EKS Helm repo..."
helm repo add eks https://aws.github.io/eks-charts 2>/dev/null || true
helm repo update

echo "Installing Gateway API CRDs..."
kubectl apply -f "https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.3.0/standard-install.yaml"

echo "Installing LBC Gateway API CRDs..."
kubectl apply -f "https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/config/crd/gateway/gateway-crds.yaml"

echo "Installing AWS Load Balancer Controller v${LBC_VERSION} with Gateway API..."
helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName="$CLUSTER_NAME" \
  --set region="$AWS_REGION" \
  --set vpcId="$VPC_ID" \
  --set serviceAccount.create=true \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set "controllerConfig.featureGates.ALBGatewayAPI=true" \
  --set "controllerConfig.featureGates.NLBGatewayAPI=true" \
  --set defaultTargetType=ip \
  --wait

echo "AWS Load Balancer Controller installed!"
kubectl get deployment -n kube-system aws-load-balancer-controller
"""

[tasks."eks:deploy:gateway"]
description = "Deploy Gateway API for ALB"
depends = ["eks:deploy:lbc"]
run = """
#!/bin/bash
set -euo pipefail

echo "Deploying Gateway API resources..."
kubectl apply -f k8s/gateway.yaml

echo "Waiting for ALB provisioning (takes ~2 minutes)..."
for i in {1..24}; do
  ALB_DNS=$(kubectl get gateway hack2future-gateway -o jsonpath='{.status.addresses[0].value}' 2>/dev/null || true)
  if [[ -n "$ALB_DNS" ]]; then
    echo ""
    echo "ALB DNS: $ALB_DNS"
    echo "API URL: http://$ALB_DNS"
    exit 0
  fi
  echo -n "."
  sleep 5
done

echo ""
echo "ALB still provisioning. Check status with:"
echo "  kubectl get gateway hack2future-gateway"
"""

[tasks."eks:gateway:status"]
description = "Get Gateway status and URL"
run = """
#!/bin/bash
set -euo pipefail
kubectl get gateway hack2future-gateway
kubectl get httproute hack2future-route
echo ""
echo "ALB DNS:"
kubectl get gateway hack2future-gateway -o jsonpath='{.status.addresses[0].value}'
echo ""
"""

[tasks."eks:deploy:ack-iam"]
description = "Install ACK IAM Controller v1.6.1"
run = """
#!/bin/bash
set -euo pipefail
ACK_IAM_VERSION="${ACK_IAM_VERSION:-v1.6.1}"
AWS_REGION="${AWS_REGION:-us-east-1}"

echo "Installing ACK IAM Controller ${ACK_IAM_VERSION}..."
helm install ack-iam-controller \
  oci://public.ecr.aws/aws-controllers-k8s/iam-chart \
  --version "$ACK_IAM_VERSION" \
  --namespace ack-system \
  --create-namespace \
  --set aws.region="$AWS_REGION" \
  --set serviceAccount.create=true \
  --set serviceAccount.name=ack-iam-controller

echo "Waiting for ACK IAM controller..."
kubectl wait --for=condition=Available deployment/ack-iam-controller \
  -n ack-system --timeout=120s

echo "ACK IAM Controller ready!"
kubectl get pods -n ack-system
"""

[tasks."pgschema:sync-configmap"]
description = "Sync database schema files to k8s configmap"
run = """
#!/bin/bash
set -euo pipefail
CONFIGMAP_ARGS="--from-file=main.sql=database/main.sql"
while IFS= read -r f; do
  key="${f#database/}"
  CONFIGMAP_ARGS="$CONFIGMAP_ARGS --from-file=$key=$f"
done < <(fd -e sql . database -E main.sql)
kubectl create configmap pgschema-database $CONFIGMAP_ARGS \
  --dry-run=client -o yaml | kubectl apply -f -
echo "Configmap pgschema-database updated"
"""

[tasks."pgschema:plan-k8s"]
description = "Preview schema changes (k8s)"
depends = ["pgschema:sync-configmap"]
run = """
#!/bin/bash
set -euo pipefail
JOB_NAME=$(kubectl create -f k8s/pgschema-update.yaml -o name)
echo "Created $JOB_NAME"
kubectl wait --for=condition=Complete --timeout=60s $JOB_NAME || true
kubectl logs $JOB_NAME
"""

[tasks."pgschema:apply-k8s"]
description = "Apply schema changes (k8s)"
depends = ["pgschema:sync-configmap"]
run = """
#!/bin/bash
set -euo pipefail
JOB_NAME=$(sed 's/value: plan/value: apply/' k8s/pgschema-update.yaml | kubectl create -f - -o name)
echo "Created $JOB_NAME"
kubectl wait --for=condition=Complete --timeout=60s $JOB_NAME || true
kubectl logs $JOB_NAME
"""

# =============================================================================
# Agents ECR Build & Push
# =============================================================================

[tasks."agents:ecr:create"]
description = "Create ECR repository for agents"
run = """
#!/bin/bash
set -euo pipefail
AWS_REGION="${AWS_REGION:-us-east-1}"
REPO_NAME="hack2future-agents"

aws ecr create-repository \
  --repository-name "$REPO_NAME" \
  --region "$AWS_REGION" \
  --image-scanning-configuration scanOnPush=true \
  --encryption-configuration encryptionType=AES256 \
  2>/dev/null || echo "Repository $REPO_NAME already exists"

echo "ECR repository ready: $REPO_NAME"
"""

[tasks."agents:build"]
description = "Build agents Docker image for ECR"
run = """
#!/bin/bash
set -euo pipefail
AWS_REGION="${AWS_REGION:-us-east-1}"
AWS_ACCOUNT_ID="${AWS_ACCOUNT_ID:-$(aws sts get-caller-identity --query Account --output text)}"
REPO_NAME="hack2future-agents"
TAG="${IMAGE_TAG:-latest}"
REGISTRY="$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com"

echo "Building $REPO_NAME:$TAG for linux/amd64..."
docker build --platform linux/amd64 -t "$REPO_NAME:$TAG" -f agents/Dockerfile .
docker tag "$REPO_NAME:$TAG" "$REGISTRY/$REPO_NAME:$TAG"

echo "Image built and tagged: $REGISTRY/$REPO_NAME:$TAG"
"""

[tasks."agents:push"]
description = "Push agents Docker image to ECR"
depends = ["ecr:login"]
run = """
#!/bin/bash
set -euo pipefail
AWS_REGION="${AWS_REGION:-us-east-1}"
AWS_ACCOUNT_ID="${AWS_ACCOUNT_ID:-$(aws sts get-caller-identity --query Account --output text)}"
REPO_NAME="hack2future-agents"
TAG="${IMAGE_TAG:-latest}"
REGISTRY="$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com"

echo "Pushing $REGISTRY/$REPO_NAME:$TAG..."
docker push "$REGISTRY/$REPO_NAME:$TAG"

echo "Image pushed successfully!"
"""

[tasks."agents:build-push"]
description = "Build and push agents Docker image to ECR"
depends = ["agents:build", "agents:push"]

[tasks."agents:deploy"]
description = "Deploy agents to EKS"
run = """
#!/bin/bash
set -euo pipefail

echo "Creating agents service account..."
kubectl apply -f k8s/agents-rbac.yaml

echo "Deploying agents..."
kubectl apply -f k8s/agents.yaml

echo "Waiting for agents deployment..."
kubectl rollout status deployment/agents --timeout=120s

echo "Agents deployed!"
kubectl get pods -l app=agents
"""

[tasks."agents:deploy:bedrock-policy"]
description = "Create Bedrock IAM policy and Pod Identity association for agents"
run = """
#!/bin/bash
set -euo pipefail
CLUSTER_NAME="${CLUSTER_NAME:-hackathon}"
AWS_REGION="${AWS_REGION:-us-east-1}"
AWS_ACCOUNT_ID="${AWS_ACCOUNT_ID:-$(aws sts get-caller-identity --query Account --output text)}"
POLICY_NAME="AgentsBedrockPolicy"

# Create IAM policy if it doesn't exist
POLICY_ARN="arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${POLICY_NAME}"
if ! aws iam get-policy --policy-arn "$POLICY_ARN" &>/dev/null; then
  echo "Creating IAM policy $POLICY_NAME..."
  aws iam create-policy --policy-name "$POLICY_NAME" --policy-document '{
    "Version": "2012-10-17",
    "Statement": [
      {
        "Sid": "BedrockInvoke",
        "Effect": "Allow",
        "Action": [
          "bedrock:InvokeModel",
          "bedrock:InvokeModelWithResponseStream"
        ],
        "Resource": [
          "arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-*",
          "arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-*"
        ]
      }
    ]
  }'
else
  echo "IAM policy $POLICY_NAME already exists"
fi

# Create Pod Identity association
echo "Creating Pod Identity association..."
eksctl create podidentityassociation \
  --cluster "$CLUSTER_NAME" \
  --region "$AWS_REGION" \
  --namespace default \
  --service-account-name agents-sa \
  --permission-policy-arns "$POLICY_ARN" \
  2>/dev/null || echo "Pod Identity association may already exist"

echo "Bedrock policy and Pod Identity configured!"
"""

[tasks."agents:logs"]
description = "View logs from agents pod"
run = """
#!/bin/bash
set -euo pipefail
kubectl logs -l app=agents --tail=100 -f
"""

# =============================================================================
# Unified Deploy/Update Tasks
# =============================================================================

[tasks."deploy:build-all"]
description = "Build all Docker images (api, frontend, agents)"
run = """
#!/bin/bash
set -euo pipefail
AWS_REGION="${AWS_REGION:-us-east-1}"
AWS_ACCOUNT_ID="${AWS_ACCOUNT_ID:-$(aws sts get-caller-identity --query Account --output text)}"
TAG="${IMAGE_TAG:-latest}"
REGISTRY="$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com"

echo "Building all images for linux/amd64..."

# Build in parallel
docker build --platform linux/amd64 -t "hack2future:$TAG" -f api/Dockerfile . &
docker build --platform linux/amd64 -t "hack2future-frontend:$TAG" -f frontend/Dockerfile . &
docker build --platform linux/amd64 -t "hack2future-agents:$TAG" -f agents/Dockerfile . &
wait

echo "Tagging images..."
docker tag "hack2future:$TAG" "$REGISTRY/hack2future:$TAG"
docker tag "hack2future-frontend:$TAG" "$REGISTRY/hack2future-frontend:$TAG"
docker tag "hack2future-agents:$TAG" "$REGISTRY/hack2future-agents:$TAG"

echo "All images built and tagged!"
"""

[tasks."deploy:push-all"]
description = "Push all Docker images to ECR"
depends = ["ecr:login", "deploy:build-all"]
run = """
#!/bin/bash
set -euo pipefail
AWS_REGION="${AWS_REGION:-us-east-1}"
AWS_ACCOUNT_ID="${AWS_ACCOUNT_ID:-$(aws sts get-caller-identity --query Account --output text)}"
TAG="${IMAGE_TAG:-latest}"
REGISTRY="$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com"

echo "Pushing all images..."
docker push "$REGISTRY/hack2future:$TAG" &
docker push "$REGISTRY/hack2future-frontend:$TAG" &
docker push "$REGISTRY/hack2future-agents:$TAG" &
wait

echo "All images pushed!"
"""

[tasks."deploy:restart"]
description = "Restart all deployments to pull latest images"
depends = ["deploy:push-all"]
run = """
#!/bin/bash
set -euo pipefail

echo "Restarting deployments..."
kubectl rollout restart deployment/hack2future deployment/hack2future-frontend deployment/agents

echo "Waiting for rollouts to complete..."
kubectl rollout status deployment/hack2future --timeout=120s &
kubectl rollout status deployment/hack2future-frontend --timeout=120s &
kubectl rollout status deployment/agents --timeout=120s &
wait

echo "All deployments restarted!"
kubectl get pods -l 'app in (hack2future, hack2future-frontend, agents)'
"""

[tasks."deploy:update"]
description = "Build, push, and restart all services (full deploy cycle)"
depends = ["deploy:restart"]

# =============================================================================
# Hotfix Tasks (fast dev iteration without rebuilding images)
# =============================================================================

[tasks."hotfix:enable"]
description = "Enable hot-reload mode on api and agents deployments for fast dev iteration"
run = """
#!/bin/bash
set -euo pipefail

echo "Enabling --reload on hack2future (api) deployment..."
kubectl patch deployment hack2future --type='json' -p='[
  {"op": "replace", "path": "/spec/template/spec/containers/0/command",
   "value": ["uv", "run", "uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]}
]'

echo "Enabling --reload on agents deployment..."
kubectl patch deployment agents --type='json' -p='[
  {"op": "replace", "path": "/spec/template/spec/containers/0/command",
   "value": ["uv", "run", "uvicorn", "agents.server:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]}
]'

echo "Waiting for rollouts..."
kubectl rollout status deployment/hack2future --timeout=120s &
kubectl rollout status deployment/agents --timeout=120s &
wait

echo ""
echo "Hot-reload enabled! Use 'mise run hotfix:api' or 'mise run hotfix:agents' to push changes."
"""

[tasks."hotfix:disable"]
description = "Disable hot-reload mode (revert to normal startup)"
run = """
#!/bin/bash
set -euo pipefail

echo "Removing custom command from hack2future (api) deployment..."
kubectl patch deployment hack2future --type='json' -p='[
  {"op": "remove", "path": "/spec/template/spec/containers/0/command"}
]' 2>/dev/null || echo "No custom command to remove"

echo "Removing custom command from agents deployment..."
kubectl patch deployment agents --type='json' -p='[
  {"op": "remove", "path": "/spec/template/spec/containers/0/command"}
]' 2>/dev/null || echo "No custom command to remove"

echo "Restarting deployments..."
kubectl rollout restart deployment/hack2future deployment/agents

echo "Waiting for rollouts..."
kubectl rollout status deployment/hack2future --timeout=120s &
kubectl rollout status deployment/agents --timeout=120s &
wait

echo "Hot-reload disabled. Normal startup restored."
"""

[tasks."hotfix:all"]
description = "Copy local changes to api, agents, and frontend pods"
depends = ["hotfix:api", "hotfix:agents", "hotfix:frontend"]

[tasks."hotfix:api"]
description = "Copy local api changes to running pod (use after hotfix:enable)"
run = """
#!/bin/bash
set -euo pipefail

POD=$(kubectl get pods -l app=hack2future --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')
if [ -z "$POD" ]; then
  echo "ERROR: No running hack2future pod found"
  exit 1
fi
echo "Target pod: $POD"

# Create a tarball and pipe it into the pod (faster than individual kubectl cp)
# COPYFILE_DISABLE=1 prevents macOS from including ._* metadata files
echo "Creating tarball of api source files..."
COPYFILE_DISABLE=1 tar -cf - \
  api/src/api/routers/*.py \
  api/src/api/*.py \
  api/src/api/queries/*.sql \
  2>/dev/null | kubectl exec -i "$POD" -c api -- tar -xf - -C /app --verbose

echo ""
echo "Files copied! Uvicorn should auto-reload if hotfix:enable was run."
echo "If not, restart manually: kubectl exec $POD -c api -- pkill -f uvicorn"
"""

[tasks."hotfix:agents"]
description = "Copy local agents changes to running pod (use after hotfix:enable)"
run = """
#!/bin/bash
set -euo pipefail

POD=$(kubectl get pods -l app=agents --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')
if [ -z "$POD" ]; then
  echo "ERROR: No running agents pod found"
  exit 1
fi
echo "Target pod: $POD"

# Create a tarball and pipe it into the pod (faster than individual kubectl cp)
# COPYFILE_DISABLE=1 prevents macOS from including ._* metadata files
echo "Creating tarball of agents source files..."
COPYFILE_DISABLE=1 tar -cf - \
  agents/src/agents/*.py \
  agents/src/agents/agent_one/*.py \
  agents/src/agents/agent_two/*.py \
  agents/src/agents/agent_three/*.py \
  2>/dev/null | kubectl exec -i "$POD" -c agents -- tar -xf - -C /app --verbose

echo ""
echo "Files copied! Uvicorn should auto-reload if hotfix:enable was run."
echo "If not, restart manually: kubectl exec $POD -c agents -- pkill -f uvicorn"
"""

[tasks."hotfix:frontend"]
description = "Build frontend locally and copy to running pod (no Docker rebuild needed)"
depends = ["frontend:build-local"]
run = """
#!/bin/bash
set -euo pipefail

POD=$(kubectl get pods -l app=hack2future-frontend --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')
if [ -z "$POD" ]; then
  echo "ERROR: No running hack2future-frontend pod found"
  exit 1
fi
echo "Target pod: $POD"

# Create a tarball and pipe it into the pod (faster than individual kubectl cp)
# COPYFILE_DISABLE=1 prevents macOS from including ._* metadata files
# Exclude config.js because it's generated at runtime from env vars
echo "Creating tarball of frontend dist files..."
COPYFILE_DISABLE=1 tar -C frontend/dist -cf - --exclude='config.js' . | \
  kubectl exec -i "$POD" -c frontend -- tar -xf - -C /usr/share/nginx/html --verbose

echo ""
echo "Frontend files copied! Nginx will serve the updated files immediately."
echo "Verify with: kubectl exec $POD -c frontend -- ls -la /usr/share/nginx/html"
"""

# =============================================================================
# Frontend Local Development Tasks
# =============================================================================

[tasks."frontend:install"]
description = "Install frontend dependencies"
dir = "frontend"
run = "npm install"

[tasks."frontend:dev"]
description = "Run frontend dev server"
depends = ["frontend:install"]
dir = "frontend"
run = "npm run dev"

[tasks."frontend:build-local"]
description = "Build frontend locally"
depends = ["frontend:install"]
dir = "frontend"
run = "npm run build"

[tasks."frontend:preview"]
description = "Preview production build locally"
depends = ["frontend:build-local"]
dir = "frontend"
run = "npm run preview"
